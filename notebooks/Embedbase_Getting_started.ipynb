{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/different-ai/embedbase/tree/main/notebooks/Embedbase_Getting_started.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "A-ImZsuRmRoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Embedbase!\n",
        "\n",
        "As a reminder, embedbase is the end-to-end platform to manage ML embeddings.\n",
        "\n",
        "Embeddings allows you to:\n",
        "- connect your data to ChatGPT or any other LLM.\n",
        "- create recommendation engines\n",
        "- classify data\n",
        "- detect anomalies\n",
        "- etc.\n",
        "\n",
        "Today we will run a local-first Embedbase using a `sentence-transformers` model as `Embedder` and a `MemoryDatabase` to store embeddings."
      ],
      "metadata": {
        "id": "kx43TO5xJAD_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vBHeL_TQI7ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429bd7d2-0edd-4abb-e94c-8a8642e87466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q embedbase sentence-transformers datasets git+https://github.com/different-ai/embedbase.git@main#subdirectory=sdk/embedbase-py httpx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "from embedbase import get_app\n",
        "from embedbase.database.memory_db import MemoryDatabase\n",
        "from embedbase.embedding.base import Embedder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class LocalEmbedder(Embedder):\n",
        "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        " \n",
        "    def __init__(\n",
        "        self, model: str = EMBEDDING_MODEL, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = SentenceTransformer(model)\n",
        " \n",
        "    @property\n",
        "    def dimensions(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the dimensions of the embeddings\n",
        "        :return: dimensions of the embeddings\n",
        "        \"\"\"\n",
        "        return self._dimensions\n",
        " \n",
        "    def is_too_big(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Check if text is too big to be embedded,\n",
        "        delegating the splitting UX to the caller\n",
        "        :param text: text to check\n",
        "        :return: True if text is too big, False otherwise\n",
        "        \"\"\"\n",
        "        return len(text) > self.model.get_max_seq_length()\n",
        " \n",
        "    async def embed(self, data: Union[List[str], str]) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Embed a list of texts\n",
        "        :param texts: list of texts\n",
        "        :return: list of embeddings\n",
        "        \"\"\"\n",
        "        embeddings = self.model.encode(data)\n",
        "        return embeddings.tolist() if isinstance(data, list) else [embeddings.tolist()]\n",
        "\n",
        "\n",
        "def run_app():\n",
        "    print(\n",
        "        \"\"\"\n",
        "                 _           _   _         _               _          _         \n",
        "        /\\ \\        /\\_\\/\\_\\ _    / /\\            /\\ \\       /\\ \\       \n",
        "       /  \\ \\      / / / / //\\_\\ / /  \\          /  \\ \\     /  \\ \\____  \n",
        "      / /\\ \\ \\    /\\ \\/ \\ \\/ / // / /\\ \\        / /\\ \\ \\   / /\\ \\_____\\ \n",
        "     / / /\\ \\_\\  /  \\____\\__/ // / /\\ \\ \\      / / /\\ \\_\\ / / /\\/___  / \n",
        "    / /_/_ \\/_/ / /\\/________// / /\\ \\_\\ \\    / /_/_ \\/_// / /   / / /  \n",
        "   / /____/\\   / / /\\/_// / // / /\\ \\ \\___\\  / /____/\\  / / /   / / /   \n",
        "  / /\\____\\/  / / /    / / // / /  \\ \\ \\__/ / /\\____\\/ / / /   / / /    \n",
        " / / /______ / / /    / / // / /____\\_\\ \\  / / /______ \\ \\ \\__/ / /     \n",
        "/ / /_______\\\\/_/    / / // / /__________\\/ / /_______\\ \\ \\___\\/ /      \n",
        "\\/__________/ _      \\/_/ \\/_____________/\\/__________/  \\/_____/       \n",
        "             / /\\            / /\\               / /\\         /\\ \\       \n",
        "            / /  \\          / /  \\             / /  \\       /  \\ \\      \n",
        "           / / /\\ \\        / / /\\ \\           / / /\\ \\__   / /\\ \\ \\     \n",
        "          / / /\\ \\ \\      / / /\\ \\ \\         / / /\\ \\___\\ / / /\\ \\_\\    \n",
        "         / / /\\ \\_\\ \\    / / /  \\ \\ \\        \\ \\ \\ \\/___// /_/_ \\/_/    \n",
        "        / / /\\ \\ \\___\\  / / /___/ /\\ \\        \\ \\ \\     / /____/\\       \n",
        "       / / /  \\ \\ \\__/ / / /_____/ /\\ \\   _    \\ \\ \\   / /\\____\\/       \n",
        "      / / /____\\_\\ \\  / /_________/\\ \\ \\ /_/\\__/ / /  / / /______       \n",
        "     / / /__________\\/ / /_       __\\ \\_\\\\ \\/___/ /  / / /_______\\      \n",
        "     \\/_____________/\\_\\___\\     /____/_/ \\_____\\/   \\/__________/      \n",
        "                                                                                                \n",
        "                 [-0.005, 0.012, -0.008, ..., -0.010]\n",
        "        \"\"\"\n",
        "    )\n",
        "    return get_app().use_db(MemoryDatabase()).use_embedder(LocalEmbedder()).run()\n",
        "\n",
        "app = run_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEduIHQnbR7Z",
        "outputId": "5b45a4d2-f3b5-4da3-c80f-0a975f3b25a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-21 15:55:24,894 - embedbase - INFO - Enabling Database <embedbase.database.memory_db.MemoryDatabase object at 0x7f44882c5d00>\n",
            "2023-04-21 15:55:24,894 - embedbase - INFO - Enabling Database <embedbase.database.memory_db.MemoryDatabase object at 0x7f44882c5d00>\n",
            "INFO:embedbase:Enabling Database <embedbase.database.memory_db.MemoryDatabase object at 0x7f44882c5d00>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                 _           _   _         _               _          _         \n",
            "        /\\ \\        /\\_\\/\\_\\ _    / /\\            /\\ \\       /\\ \\       \n",
            "       /  \\ \\      / / / / //\\_\\ / /  \\          /  \\ \\     /  \\ \\____  \n",
            "      / /\\ \\ \\    /\\ \\/ \\ \\/ / // / /\\ \\        / /\\ \\ \\   / /\\ \\_____\\ \n",
            "     / / /\\ \\_\\  /  \\____\\__/ // / /\\ \\ \\      / / /\\ \\_\\ / / /\\/___  / \n",
            "    / /_/_ \\/_/ / /\\/________// / /\\ \\_\\ \\    / /_/_ \\/_// / /   / / /  \n",
            "   / /____/\\   / / /\\/_// / // / /\\ \\ \\___\\  / /____/\\  / / /   / / /   \n",
            "  / /\\____\\/  / / /    / / // / /  \\ \\ \\__/ / /\\____\\/ / / /   / / /    \n",
            " / / /______ / / /    / / // / /____\\_\\ \\  / / /______ \\ \\ \\__/ / /     \n",
            "/ / /_______\\/_/    / / // / /__________\\/ / /_______\\ \\ \\___\\/ /      \n",
            "\\/__________/ _      \\/_/ \\/_____________/\\/__________/  \\/_____/       \n",
            "             / /\\            / /\\               / /\\         /\\ \\       \n",
            "            / /  \\          / /  \\             / /  \\       /  \\ \\      \n",
            "           / / /\\ \\        / / /\\ \\           / / /\\ \\__   / /\\ \\ \\     \n",
            "          / / /\\ \\ \\      / / /\\ \\ \\         / / /\\ \\___\\ / / /\\ \\_\\    \n",
            "         / / /\\ \\_\\ \\    / / /  \\ \\ \\        \\ \\ \\ \\/___// /_/_ \\/_/    \n",
            "        / / /\\ \\ \\___\\  / / /___/ /\\ \\        \\ \\ \\     / /____/\\       \n",
            "       / / /  \\ \\ \\__/ / / /_____/ /\\ \\   _    \\ \\ \\   / /\\____\\/       \n",
            "      / / /____\\_\\ \\  / /_________/\\ \\ \\ /_/\\__/ / /  / / /______       \n",
            "     / / /__________\\/ / /_       __\\ \\_\\ \\/___/ /  / / /_______\\      \n",
            "     \\/_____________/\\_\\___\\     /____/_/ \\_____\\/   \\/__________/      \n",
            "                                                                                                \n",
            "                 [-0.005, 0.012, -0.008, ..., -0.010]\n",
            "        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-21 15:55:25,428 - embedbase - INFO - Enabling Embedder <__main__.LocalEmbedder object at 0x7f45741c5220>\n",
            "2023-04-21 15:55:25,428 - embedbase - INFO - Enabling Embedder <__main__.LocalEmbedder object at 0x7f45741c5220>\n",
            "INFO:embedbase:Enabling Embedder <__main__.LocalEmbedder object at 0x7f45741c5220>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset_id = \"fka/awesome-chatgpt-prompts\"\n",
        "dataset = load_dataset(dataset_id, 'en', split='train', streaming=True)\n",
        "print(next(iter(dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai8VyitBfgYR",
        "outputId": "66b60fa0-07db-4d3a-c1d4-b435ba339b58"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'act': 'Linux Terminal', 'prompt': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from embedbase_client.split import split_text\n",
        "from httpx import AsyncClient\n",
        "import json\n",
        "\n",
        "embedbase_dataset_id = dataset_id.split(\"/\")[-1]\n",
        "i = 0\n",
        "documents = []\n",
        "for row in dataset:\n",
        "  for c in split_text(row[\"prompt\"], max_tokens=30, chunk_overlap=20):\n",
        "    documents.append({\n",
        "        \"data\": c.chunk,\n",
        "    })\n",
        "\n",
        "  i+=1\n",
        "  if i > 100_000:\n",
        "    break\n",
        "async with AsyncClient(app=app, base_url=\"http://localhost:8000\") as client:\n",
        "  res = await client.post(f\"/v1/{embedbase_dataset_id}\", json={\"documents\": documents})\n",
        "  print(res.json())"
      ],
      "metadata": {
        "id": "ImWjEm1aKPDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96875e5c-ce51-4d0b-e156-44111401ff73"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-21 16:06:27,493 - embedbase - INFO - Refreshing 1476 embeddings\n",
            "2023-04-21 16:06:27,493 - embedbase - INFO - Refreshing 1476 embeddings\n",
            "INFO:embedbase:Refreshing 1476 embeddings\n",
            "2023-04-21 16:06:27,519 - embedbase - INFO - Checking embeddings computing necessity for 1476 documents\n",
            "2023-04-21 16:06:27,519 - embedbase - INFO - Checking embeddings computing necessity for 1476 documents\n",
            "INFO:embedbase:Checking embeddings computing necessity for 1476 documents\n",
            "2023-04-21 16:06:27,595 - embedbase - INFO - We will compute embeddings for 1476/1476 documents\n",
            "2023-04-21 16:06:27,595 - embedbase - INFO - We will compute embeddings for 1476/1476 documents\n",
            "INFO:embedbase:We will compute embeddings for 1476/1476 documents\n",
            "2023-04-21 16:06:53,615 - embedbase - INFO - Uploaded 1443 documents\n",
            "2023-04-21 16:06:53,615 - embedbase - INFO - Uploaded 1443 documents\n",
            "INFO:embedbase:Uploaded 1443 documents\n",
            "2023-04-21 16:06:53,624 - embedbase - INFO - Uploaded in 26.130375623703003 seconds\n",
            "2023-04-21 16:06:53,624 - embedbase - INFO - Uploaded in 26.130375623703003 seconds\n",
            "INFO:embedbase:Uploaded in 26.130375623703003 seconds\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from httpx import AsyncClient\n",
        "\n",
        "async with AsyncClient(app=app, base_url=\"http://localhost:8000\") as client:\n",
        "  res = await client.post(f\"/v1/{embedbase_dataset_id}/search\", json={\"query\": \"any idea of recipes with lentils with basil and ginger?\", \"top_k\": 15})\n",
        "  res = res.json()\n",
        "  for r in res[\"similarities\"]:\n",
        "    print(r[\"data\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sMu2mNQjMym",
        "outputId": "8f4a2896-0986-4caf-9090-21a68da84977"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-21 16:08:23,395 - embedbase - INFO - Query any idea of recipes with lentils with basil and ginger? created embedding, querying index\n",
            "2023-04-21 16:08:23,395 - embedbase - INFO - Query any idea of recipes with lentils with basil and ginger? created embedding, querying index\n",
            "INFO:embedbase:Query any idea of recipes with lentils with basil and ginger? created embedding, querying index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", and you will suggest recipes for me to try. You should only reply with the recipes you recommend, and nothing else. Do not write explanations.\n",
            ". You should only reply with the recipes you recommend, and nothing else. Do not write explanations. My first request is \"I am a vegetarian and\n",
            " I will tell you about my dietary preferences and allergies, and you will suggest recipes for me to try. You should only reply with the recipes you recommend\n",
            " I am looking for healthy dinner ideas.\"\n",
            " a vegetarian recipe for 2 people that has approximate 500 calories per serving and has a low glycemic index. Can you please provide a suggestion?\n",
            "I want you to act as my personal chef. I will tell you about my dietary preferences and allergies, and you will suggest recipes for me to try\n",
            ", and nothing else. Do not write explanations. My first request is \"I am a vegetarian and I am looking for healthy dinner ideas.\"\n",
            "I require someone who can suggest delicious recipes that includes foods which are nutritionally beneficial but also easy & not time consuming enough therefore suitable for busy people like\n",
            " My first request is \"I am a vegetarian and I am looking for healthy dinner ideas.\"\n",
            "As a dietitian, I would like to design a vegetarian recipe for 2 people that has approximate 500 calories per serving and has a low glyc\n",
            "I want you to act as a doctor and come up with creative treatments for illnesses or diseases. You should be able to recommend conventional medicines, herbal remedies\n",
            " have any insights concerning this particular type of green tea organic blend ?\"\n",
            " based upon flavor profile tasting them carefully then reporting it back in jargon used by connoisseurs in order figure out what's unique about any given\n",
            " up with creative treatments for illnesses or diseases. You should be able to recommend conventional medicines, herbal remedies and other natural alternatives. You will also need to\n",
            " should be able to recommend conventional medicines, herbal remedies and other natural alternatives. You will also need to consider the patient’s age, lifestyle and medical history\n"
          ]
        }
      ]
    }
  ]
}