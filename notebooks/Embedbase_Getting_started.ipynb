{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f37903d90dca4dc6b0448ce83d54ffc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_554201457b464f9d82a53b463d9dddc7",
              "IPY_MODEL_27913bcafa0b438da07d913cdf66bc97",
              "IPY_MODEL_2e147b87d18545f1b6621f98a7520d80"
            ],
            "layout": "IPY_MODEL_750722402ccd426b83360cb00a7db65f"
          }
        },
        "554201457b464f9d82a53b463d9dddc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f20b09e173e2469a935fdec69c5c3acf",
            "placeholder": "​",
            "style": "IPY_MODEL_e52e8f754d5e47a5af2a632dca081955",
            "value": "Downloading readme: 100%"
          }
        },
        "27913bcafa0b438da07d913cdf66bc97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a182d505e3a94a6e944bd47b13fb3aa2",
            "max": 274,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aefba5b2f1b746e29b418c4f03d8c58e",
            "value": 274
          }
        },
        "2e147b87d18545f1b6621f98a7520d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0a5697796854b458790c77734ea9300",
            "placeholder": "​",
            "style": "IPY_MODEL_87784131814b44738a04a464f97c5c87",
            "value": " 274/274 [00:00&lt;00:00, 17.0kB/s]"
          }
        },
        "750722402ccd426b83360cb00a7db65f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f20b09e173e2469a935fdec69c5c3acf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e52e8f754d5e47a5af2a632dca081955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a182d505e3a94a6e944bd47b13fb3aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aefba5b2f1b746e29b418c4f03d8c58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0a5697796854b458790c77734ea9300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87784131814b44738a04a464f97c5c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/different-ai/embedbase/blob/main/notebooks/Embedbase_Getting_started.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "A-ImZsuRmRoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Embedbase!\n",
        "\n",
        "![embedbase logo](https://docs.embedbase.xyz/embedbase-long.svg)\n",
        "\n",
        "As a reminder, embedbase is the end-to-end platform to manage ML embeddings.\n",
        "\n",
        "Embeddings allows you to:\n",
        "- connect your data to ChatGPT or any other LLM.\n",
        "- create recommendation engines\n",
        "- classify data\n",
        "- detect anomalies\n",
        "- etc.\n",
        "\n",
        "Today we will run a local-first Embedbase using a `sentence-transformers` model as `Embedder` and a `MemoryDatabase` to store embeddings."
      ],
      "metadata": {
        "id": "kx43TO5xJAD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to install a few dependencies, such as\n",
        "- [Huggingface's \"datasets\" library](https://huggingface.co/docs/datasets/index) to get some real data to play with"
      ],
      "metadata": {
        "id": "q_KE8xuB-gl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vBHeL_TQI7ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda60c33-213a-46cf-9d7a-7ea60f922470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for embedbase-client (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q embedbase sentence-transformers datasets git+https://github.com/different-ai/embedbase.git@main#subdirectory=sdk/embedbase-py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up an \"Embedder\", a key component of embedbase that transforms data into vectors allowing to compare similarities.\n",
        "\n",
        "Another key component of embedbase is the `database` where you store your data and these vectors. In this example we will store everything in memory. \n",
        "\n",
        "ℹ️ In production you should use a database that supports vectors. Embedbase Cloud uses Supabase for example."
      ],
      "metadata": {
        "id": "AikQzBTvEP5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "from embedbase import get_app\n",
        "from embedbase.database.memory_db import MemoryDatabase\n",
        "from embedbase.embedding.base import Embedder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class LocalEmbedder(Embedder):\n",
        "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        " \n",
        "    def __init__(\n",
        "        self, model: str = EMBEDDING_MODEL, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = SentenceTransformer(model)\n",
        " \n",
        "    @property\n",
        "    def dimensions(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the dimensions of the embeddings\n",
        "        :return: dimensions of the embeddings\n",
        "        \"\"\"\n",
        "        return self._dimensions\n",
        " \n",
        "    def is_too_big(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Check if text is too big to be embedded,\n",
        "        delegating the splitting UX to the caller\n",
        "        :param text: text to check\n",
        "        :return: True if text is too big, False otherwise\n",
        "        \"\"\"\n",
        "        return len(text) > self.model.get_max_seq_length()\n",
        " \n",
        "    async def embed(self, data: Union[List[str], str]) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Embed a list of texts\n",
        "        :param texts: list of texts\n",
        "        :return: list of embeddings\n",
        "        \"\"\"\n",
        "        embeddings = self.model.encode(data)\n",
        "        return embeddings.tolist() if isinstance(data, list) else [embeddings.tolist()]\n",
        "\n",
        "\n",
        "def run_app():\n",
        "    print(\n",
        "        \"\"\"\n",
        "                 _           _   _         _               _          _         \n",
        "        /\\ \\        /\\_\\/\\_\\ _    / /\\            /\\ \\       /\\ \\       \n",
        "       /  \\ \\      / / / / //\\_\\ / /  \\          /  \\ \\     /  \\ \\____  \n",
        "      / /\\ \\ \\    /\\ \\/ \\ \\/ / // / /\\ \\        / /\\ \\ \\   / /\\ \\_____\\ \n",
        "     / / /\\ \\_\\  /  \\____\\__/ // / /\\ \\ \\      / / /\\ \\_\\ / / /\\/___  / \n",
        "    / /_/_ \\/_/ / /\\/________// / /\\ \\_\\ \\    / /_/_ \\/_// / /   / / /  \n",
        "   / /____/\\   / / /\\/_// / // / /\\ \\ \\___\\  / /____/\\  / / /   / / /   \n",
        "  / /\\____\\/  / / /    / / // / /  \\ \\ \\__/ / /\\____\\/ / / /   / / /    \n",
        " / / /______ / / /    / / // / /____\\_\\ \\  / / /______ \\ \\ \\__/ / /     \n",
        "/ / /_______\\\\/_/    / / // / /__________\\/ / /_______\\ \\ \\___\\/ /      \n",
        "\\/__________/ _      \\/_/ \\/_____________/\\/__________/  \\/_____/       \n",
        "             / /\\            / /\\               / /\\         /\\ \\       \n",
        "            / /  \\          / /  \\             / /  \\       /  \\ \\      \n",
        "           / / /\\ \\        / / /\\ \\           / / /\\ \\__   / /\\ \\ \\     \n",
        "          / / /\\ \\ \\      / / /\\ \\ \\         / / /\\ \\___\\ / / /\\ \\_\\    \n",
        "         / / /\\ \\_\\ \\    / / /  \\ \\ \\        \\ \\ \\ \\/___// /_/_ \\/_/    \n",
        "        / / /\\ \\ \\___\\  / / /___/ /\\ \\        \\ \\ \\     / /____/\\       \n",
        "       / / /  \\ \\ \\__/ / / /_____/ /\\ \\   _    \\ \\ \\   / /\\____\\/       \n",
        "      / / /____\\_\\ \\  / /_________/\\ \\ \\ /_/\\__/ / /  / / /______       \n",
        "     / / /__________\\/ / /_       __\\ \\_\\\\ \\/___/ /  / / /_______\\      \n",
        "     \\/_____________/\\_\\___\\     /____/_/ \\_____\\/   \\/__________/      \n",
        "                                                                                                \n",
        "                 [-0.005, 0.012, -0.008, ..., -0.010]\n",
        "        \"\"\"\n",
        "    )\n",
        "    return get_app().use_db(MemoryDatabase()).use_embedder(LocalEmbedder()).run()\n",
        "\n",
        "app = run_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEduIHQnbR7Z",
        "outputId": "d3049c34-6828-4558-b6db-15a3c27e7224"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-23 12:24:46,622 - embedbase - INFO - Enabling Database <embedbase.database.memory_db.MemoryDatabase object at 0x7f36ec232be0>\n",
            "2023-04-23 12:24:46,622 - embedbase - INFO - Enabling Database <embedbase.database.memory_db.MemoryDatabase object at 0x7f36ec232be0>\n",
            "INFO:embedbase:Enabling Database <embedbase.database.memory_db.MemoryDatabase object at 0x7f36ec232be0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                 _           _   _         _               _          _         \n",
            "        /\\ \\        /\\_\\/\\_\\ _    / /\\            /\\ \\       /\\ \\       \n",
            "       /  \\ \\      / / / / //\\_\\ / /  \\          /  \\ \\     /  \\ \\____  \n",
            "      / /\\ \\ \\    /\\ \\/ \\ \\/ / // / /\\ \\        / /\\ \\ \\   / /\\ \\_____\\ \n",
            "     / / /\\ \\_\\  /  \\____\\__/ // / /\\ \\ \\      / / /\\ \\_\\ / / /\\/___  / \n",
            "    / /_/_ \\/_/ / /\\/________// / /\\ \\_\\ \\    / /_/_ \\/_// / /   / / /  \n",
            "   / /____/\\   / / /\\/_// / // / /\\ \\ \\___\\  / /____/\\  / / /   / / /   \n",
            "  / /\\____\\/  / / /    / / // / /  \\ \\ \\__/ / /\\____\\/ / / /   / / /    \n",
            " / / /______ / / /    / / // / /____\\_\\ \\  / / /______ \\ \\ \\__/ / /     \n",
            "/ / /_______\\/_/    / / // / /__________\\/ / /_______\\ \\ \\___\\/ /      \n",
            "\\/__________/ _      \\/_/ \\/_____________/\\/__________/  \\/_____/       \n",
            "             / /\\            / /\\               / /\\         /\\ \\       \n",
            "            / /  \\          / /  \\             / /  \\       /  \\ \\      \n",
            "           / / /\\ \\        / / /\\ \\           / / /\\ \\__   / /\\ \\ \\     \n",
            "          / / /\\ \\ \\      / / /\\ \\ \\         / / /\\ \\___\\ / / /\\ \\_\\    \n",
            "         / / /\\ \\_\\ \\    / / /  \\ \\ \\        \\ \\ \\ \\/___// /_/_ \\/_/    \n",
            "        / / /\\ \\ \\___\\  / / /___/ /\\ \\        \\ \\ \\     / /____/\\       \n",
            "       / / /  \\ \\ \\__/ / / /_____/ /\\ \\   _    \\ \\ \\   / /\\____\\/       \n",
            "      / / /____\\_\\ \\  / /_________/\\ \\ \\ /_/\\__/ / /  / / /______       \n",
            "     / / /__________\\/ / /_       __\\ \\_\\ \\/___/ /  / / /_______\\      \n",
            "     \\/_____________/\\_\\___\\     /____/_/ \\_____\\/   \\/__________/      \n",
            "                                                                                                \n",
            "                 [-0.005, 0.012, -0.008, ..., -0.010]\n",
            "        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-23 12:24:46,832 - embedbase - INFO - Enabling Embedder <__main__.LocalEmbedder object at 0x7f37d7d0c5b0>\n",
            "2023-04-23 12:24:46,832 - embedbase - INFO - Enabling Embedder <__main__.LocalEmbedder object at 0x7f37d7d0c5b0>\n",
            "INFO:embedbase:Enabling Embedder <__main__.LocalEmbedder object at 0x7f37d7d0c5b0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the [dataset of chatgpt prompts](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts)"
      ],
      "metadata": {
        "id": "DuqbImGO-26H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset_id = \"fka/awesome-chatgpt-prompts\"\n",
        "dataset = load_dataset(dataset_id, 'en', split='train', streaming=True)\n",
        "print(next(iter(dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "f37903d90dca4dc6b0448ce83d54ffc2",
            "554201457b464f9d82a53b463d9dddc7",
            "27913bcafa0b438da07d913cdf66bc97",
            "2e147b87d18545f1b6621f98a7520d80",
            "750722402ccd426b83360cb00a7db65f",
            "f20b09e173e2469a935fdec69c5c3acf",
            "e52e8f754d5e47a5af2a632dca081955",
            "a182d505e3a94a6e944bd47b13fb3aa2",
            "aefba5b2f1b746e29b418c4f03d8c58e",
            "b0a5697796854b458790c77734ea9300",
            "87784131814b44738a04a464f97c5c87"
          ]
        },
        "id": "ai8VyitBfgYR",
        "outputId": "47409bca-eb61-4a10-8092-6deb3ebcbbea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/274 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f37903d90dca4dc6b0448ce83d54ffc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'act': 'Linux Terminal', 'prompt': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a necessary hack when you want to use EmbedbaseClient in Jupyter notebook like colab\n",
        "# https://stackoverflow.com/questions/46827007/runtimeerror-this-event-loop-is-already-running-in-python\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "d3yEu1bF-GSH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from embedbase_client.client import EmbedbaseClient\n",
        "from embedbase_client.split import split_text\n",
        "from pprint import pprint\n",
        "\n",
        "embedbase = EmbedbaseClient(\"http://localhost:8000\", fastapi_app=app)\n",
        "\n",
        "embedbase_dataset_id = dataset_id.split(\"/\")[-1]\n",
        "documents = []\n",
        "for row in dataset:\n",
        "  # ⚠️ note here that we split in small chunks of max_tokens \"30\" because\n",
        "  # the model used has a relatively limited input size\n",
        "  # when using other models such as OpenAI's embeddings model, you can\n",
        "  # use max_tokens of 500 and chunk_overlap of 200 for example\n",
        "  # (embedbase cloud use openai model at the moment) ⚠️\n",
        "  for c in split_text(row[\"prompt\"], max_tokens=30, chunk_overlap=20):\n",
        "    documents.append({\n",
        "        \"data\": c.chunk,\n",
        "    })\n",
        "\n",
        "res = embedbase.dataset(embedbase_dataset_id).batch_add(documents)\n",
        "pprint(res[0:5])"
      ],
      "metadata": {
        "id": "ImWjEm1aKPDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b04515-75af-4de4-9fbc-caad46f9f717"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-23 12:34:01,658 - embedbase - INFO - Refreshing 1476 embeddings\n",
            "2023-04-23 12:34:01,658 - embedbase - INFO - Refreshing 1476 embeddings\n",
            "INFO:embedbase:Refreshing 1476 embeddings\n",
            "2023-04-23 12:34:01,666 - embedbase - INFO - Checking embeddings computing necessity for 1476 documents\n",
            "2023-04-23 12:34:01,666 - embedbase - INFO - Checking embeddings computing necessity for 1476 documents\n",
            "INFO:embedbase:Checking embeddings computing necessity for 1476 documents\n",
            "2023-04-23 12:34:01,707 - embedbase - INFO - We will compute embeddings for 1476/1476 documents\n",
            "2023-04-23 12:34:01,707 - embedbase - INFO - We will compute embeddings for 1476/1476 documents\n",
            "INFO:embedbase:We will compute embeddings for 1476/1476 documents\n",
            "2023-04-23 12:34:17,606 - embedbase - INFO - Uploaded 1476 documents\n",
            "2023-04-23 12:34:17,606 - embedbase - INFO - Uploaded 1476 documents\n",
            "INFO:embedbase:Uploaded 1476 documents\n",
            "2023-04-23 12:34:17,610 - embedbase - INFO - Uploaded in 15.952126264572144 seconds\n",
            "2023-04-23 12:34:17,610 - embedbase - INFO - Uploaded in 15.952126264572144 seconds\n",
            "INFO:embedbase:Uploaded in 15.952126264572144 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '27e1404fd1cd74b065ebd5984e0578dab9d005f10f7a9ac514122debe894b7e7',\n",
            "  'status': 'success'},\n",
            " {'id': '4412fafe70a5d744e502e859ef4326a635c70cddda7ea005f217c7d2a6b9cd46',\n",
            "  'status': 'success'},\n",
            " {'id': '8c5149b60a4f16266abd52393ee9029f1d367f3a79eb9ef43b2ce8686fe3ceb1',\n",
            "  'status': 'success'},\n",
            " {'id': '728dbf6ec7ee429feb3ed787c351f72d4680cb9d166e2cb60d7c612948d88e24',\n",
            "  'status': 'success'},\n",
            " {'id': 'dbd9e5692642a4bd7d69485097764ad95a5e19e446a7181f384e576bb263bc41',\n",
            "  'status': 'success'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = embedbase.dataset(embedbase_dataset_id).search(\"historian persona, expert in cultural, economic, political, and social events\", 15)\n",
        "for r in res:\n",
        "    pprint(r.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sMu2mNQjMym",
        "outputId": "c81d1cb0-85b6-4618-d9fc-a75dea891e11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-23 12:35:36,621 - embedbase - INFO - Query historian persona, expert in cultural, economic, political, and social events created embedding, querying index\n",
            "2023-04-23 12:35:36,621 - embedbase - INFO - Query historian persona, expert in cultural, economic, political, and social events created embedding, querying index\n",
            "INFO:embedbase:Query historian persona, expert in cultural, economic, political, and social events created embedding, querying index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I want you to act as a historian. You will research and analyze cultural, '\n",
            " 'economic, political, and social events in the past, collect data from')\n",
            "(' and social events in the past, collect data from primary sources and use it '\n",
            " 'to develop theories about what happened during various periods of history. '\n",
            " 'My first suggestion')\n",
            "(' will research and analyze cultural, economic, political, and social events '\n",
            " 'in the past, collect data from primary sources and use it to develop '\n",
            " 'theories about what')\n",
            "(' primary sources and use it to develop theories about what happened during '\n",
            " 'various periods of history. My first suggestion request is \"I need help '\n",
            " 'uncovering facts about')\n",
            "('I want you to act as my time travel guide. I will provide you with the '\n",
            " 'historical period or future time I want to visit and you will suggest')\n",
            "' suggest some interesting events, sights, or people for me to experience?\"'\n",
            "('. I will provide you with the historical period or future time I want to '\n",
            " 'visit and you will suggest the best events, sights, or people to experience')\n",
            "('. I will provide you with some topics related to current events and you will '\n",
            " 'use your wit, creativity, and observational skills to create a routine based '\n",
            " 'on')\n",
            "('I want you to act as a mathematical history teacher and provide information '\n",
            " 'about the historical development of mathematical concepts and the '\n",
            " 'contributions of different mathematicians. You should')\n",
            "(' the best events, sights, or people to experience. Do not write '\n",
            " 'explanations, simply provide the suggestions and any necessary information. '\n",
            " 'My first request is \"')\n",
            "(' current events and you will use your wit, creativity, and observational '\n",
            " 'skills to create a routine based on those topics. You should also be sure to '\n",
            " 'incorporate')\n",
            "('I want you to act as a debater. I will provide you with some topics related '\n",
            " 'to current events and your task is to research both sides of')\n",
            "(' I will provide you with some topics related to current events and your task '\n",
            " 'is to research both sides of the debates, present valid arguments for each '\n",
            " 'side,')\n",
            "(' session e.g., if it’s children then you can talk about animals; If it’s '\n",
            " 'adults then history-based tales might engage them better etc. My')\n",
            "('I want you to act as an academician. You will be responsible for researching '\n",
            " 'a topic of your choice and presenting the findings in a paper or article')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats, you saw the main features of embedbase, from this, you can build:\n",
        "- A [recommendation engine](https://betterprogramming.pub/using-openai-to-increase-time-spent-on-your-blog-3f138d5ae6aa)\n",
        "- Connect your data sources to ChatGPT/LLMs, for example, for [a chatgpt powered documentation](https://betterprogramming.pub/building-a-chatgpt-powered-markdown-documentation-in-no-time-50e308f9038e)\n",
        "- Detect anomalies\n",
        "- Classify your data\n",
        "- Vizualize your data distribution in 2D or 3D"
      ],
      "metadata": {
        "id": "K6fLKTaP_eoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting the dot with LLMs like ChatGPT\n",
        "\n",
        "ChatGPT is a game-changer, but it doesn't have any knowledge about your company, your product, or your data. Embedbase makes it easy for you to connect any data sources to ChatGPT."
      ],
      "metadata": {
        "id": "VhN8Hv00C0yL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will plug our dataset of prompts to chatgpt so that it can answer questions about it.\n",
        "\n",
        "We will need an OpenAI API key that you can get at\n",
        "https://platform.openai.com/account/api-keys.\n",
        "\n",
        "Then we can install the OpenAI SDK:"
      ],
      "metadata": {
        "id": "vmI8X8dlC7x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "tQFYdKGKDBEI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Large language models such as ChatGPT, GPT-4 and others are limited in input size, so we need to give them the best information to solve the user's problem.\n",
        "\n",
        "A good question create a good answer, and that's where embedbase shines!\n",
        "\n",
        "We will use the function `merge` from `embedbase_client` in order to merge the search results into a string that can fit into the AI input."
      ],
      "metadata": {
        "id": "RBEGu5SYDEEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from embedbase_client.split import merge\n",
        "\n",
        "openai.api_key = \"<https://platform.openai.com/account/api-keys>\"\n",
        "openai.api_key = \"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\"\n",
        "def build_prompt(question: str, context: str):\n",
        "  return f\"Based on the following context:\\n{context}\\nAnswer the user's question: {question}\"\n",
        "\n",
        "question = \"Provide me a persona that is 'historian persona, expert in cultural, economic, political, and social events' but also contrarian\"\n",
        "results = embedbase.dataset(embedbase_dataset_id).search(question, limit=5)\n",
        "merged_results = merge([result.data for result in results])\n",
        "persona = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about a list of chatgpt prompts\"},\n",
        "        {\"role\": \"user\", \"content\": build_prompt(question, merged_results)}\n",
        "    ]\n",
        ")[\"choices\"][0][\"message\"][\"content\"]\n",
        "persona"
      ],
      "metadata": {
        "id": "zk0aGYODC0CE",
        "outputId": "e56529fd-b4d8-4432-e50b-63527a2167ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-23 12:42:29,807 - embedbase - INFO - Query Provide me a persona that is 'historian persona, expert in cultural, economic, political, and social events' but also contrarian created embedding, querying index\n",
            "2023-04-23 12:42:29,807 - embedbase - INFO - Query Provide me a persona that is 'historian persona, expert in cultural, economic, political, and social events' but also contrarian created embedding, querying index\n",
            "INFO:embedbase:Query Provide me a persona that is 'historian persona, expert in cultural, economic, political, and social events' but also contrarian created embedding, querying index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure! Meet Dr. Julia Martinez. Julia is a highly respected historian, specializing in cultural, economic, political, and social events throughout history. She's published numerous papers, lectured at prestigious universities, and consulted for various historical organizations. However, Julia is also known as a contrarian - she enjoys questioning traditional theories and perspectives, often challenging the status quo. Her unique approach to analyzing historical events provides a fresh perspective that has made her a sought-after expert in the field.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's use this persona to ask a question about Jean-Francois de La Perouse"
      ],
      "metadata": {
        "id": "4qKc0uT_Iz6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": f\"You are a powerful AI assistant with this personality: '{persona}'\"},\n",
        "        {\"role\": \"user\", \"content\": \"List me the main discoveries of Jean-Francois de La Perouse and tell me a bit about his personality\"}\n",
        "    ]\n",
        ")[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "JaCtG9poIkU3",
        "outputId": "8c1308ab-3f54-4517-85ae-6fd77d74fa76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Jean-Francois de La Perouse was a French explorer and naval officer who led '\n",
            " 'the voyage of the frigates Astrolabe and Boussole in the late 18th century. '\n",
            " 'Here are some of his main discoveries:\\n'\n",
            " '\\n'\n",
            " '1. Mapping the coast of Alaska: La Perouse explored the Alaskan coast and '\n",
            " 'produced the first accurate map of the area.\\n'\n",
            " '\\n'\n",
            " '2. Discovering the island of Maui: During his travels in the Pacific, La '\n",
            " 'Perouse discovered the Hawaiian island of Maui and named it \"Isle de la '\n",
            " 'Caimane.\"\\n'\n",
            " '\\n'\n",
            " '3. Studying the indigenous peoples of Alaska and the Pacific: La Perouse was '\n",
            " 'known for his detailed research on the native peoples of the regions he '\n",
            " 'explored, including their languages, customs, and religions.\\n'\n",
            " '\\n'\n",
            " 'As for his personality, La Perouse was known as a methodical and meticulous '\n",
            " 'explorer who paid great attention to detail. He was also respected for his '\n",
            " 'humanitarianism, as he often went out of his way to help the native peoples '\n",
            " 'he encountered on his expeditions. However, he could also be arrogant and '\n",
            " 'argumentative, and clashed with some of his crew members at times. Overall, '\n",
            " 'he was a complex and fascinating figure in the history of exploration.')\n"
          ]
        }
      ]
    }
  ]
}